{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWlri/OCP95n/zHIxxS0fA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OfriA/AppliedProject52056/blob/main/data/DataExploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Feiva6BRoIdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "url = \"https://github.com/OfriA/AppliedProject52056/raw/refs/heads/main/data/ER_data.xlsx\"\n",
        "data = pd.read_excel(url)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "GiyVQAHDmj9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "id": "HL1NJrGim6Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[['Progress', 'Duration (in seconds)', 'sex', 'age', 'ed_level', 'ses', 'DAS_1', 'custody', 'n_children', 'child_age', 'child_gender', 'moved_Y/N', 'therapy', 'psych_drugs', 'health', 'partner_duty', 'alarms_freq', ]].describe()"
      ],
      "metadata": {
        "id": "2uontPRBmzOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.age.hist(bins=20)"
      ],
      "metadata": {
        "id": "iyKSf9cKpHuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.n_children.hist()"
      ],
      "metadata": {
        "id": "bIJiYtDQq3gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "DC7MyrHvtV7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptive_corr = data[['sex', 'age', 'ed_level', 'ses', 'DAS_1', 'custody', 'n_children', 'child_age', 'child_gender', 'moved_Y/N', 'therapy', 'psych_drugs', 'health', 'partner_duty', 'alarms_freq', ]].corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(8, 6)) # Adjust figure size as needed\n",
        "sns.heatmap(descriptive_corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Descriptive Statistics - Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5x7BvYi6tVhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# War exposure features\n",
        "self_exposure_features = ['SelfExposure_1', 'SelfExposure_2', 'SelfExposure_3', 'SelfExposure_4', 'SelfExposure_5', 'SelfExposure_6']\n",
        "other_exposure_features = ['OtherExposure_6', 'OtherExposure_7', 'OtherExposure_8', 'OtherExposure_9', 'OtherExposure_10', 'OtherExposure_11', 'OtherExposure_12']\n",
        "\n",
        "# Commpute war exposure score\n",
        "data['self_exposure_score'] = np.sum(data[self_exposure_features], axis=1)\n",
        "data['other_exposure_score'] = np.sum(data[other_exposure_features], axis=1)\n",
        "data['war_exposure_score'] = np.sum(data[['self_exposure_score', 'other_exposure_score']], axis=1)"
      ],
      "metadata": {
        "id": "gdnOzADYj8lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CBCL features\n",
        "CBCL_features = data.columns[240:283].to_list()\n",
        "\n",
        "CBCL_D_features = CBCL_features[0:13]\n",
        "CBCL_A_features = CBCL_features[13:31]\n",
        "CBCL_S_features = CBCL_features[31:]\n",
        "\n",
        "\n",
        "# Compute CBCL score\n",
        "data['CBCL_D_score'] = np.sum(data[CBCL_D_features], axis=1)\n",
        "data['CBCL_A_score'] = np.sum(data[CBCL_A_features], axis=1)\n",
        "data['CBCL_S_score'] = np.sum(data[CBCL_S_features], axis=1)\n",
        "data['CBCL_score'] = np.sum(data[['CBCL_D_score', 'CBCL_A_score', 'CBCL_S_score']], axis=1)"
      ],
      "metadata": {
        "id": "VZHWJeSKSkwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "NEwV9jTvYjG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute length of texts\n",
        "data['Event_length'] = data['Event'].str.len()\n",
        "data['EER_text_length'] = data['EER_text'].str.len()\n",
        "\n",
        "data['EER_text_length'] = data['EER_text_length'].fillna(0)\n"
      ],
      "metadata": {
        "id": "SnzbX-mdYexR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Event length mean: {data['Event_length'].mean():.5}, Event length SD: {data['Event_length'].std():.5}\")\n",
        "print(f\"EER text length mean: {data['EER_text_length'].mean():.5}, EER text length SD: {data['EER_text_length'].std():.5}\")"
      ],
      "metadata": {
        "id": "kXNQfTn1uoM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=data, x='sex', y='Event_length')\n",
        "plt.title(\"Text Length by Sex\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ie7HX25yUn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Omitting NA's\n",
        "data = data[np.sum(data[self_exposure_features + other_exposure_features + CBCL_features].isna(), axis=1) == 0]"
      ],
      "metadata": {
        "id": "asvl9j4wwDka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "DTiv2I2g78-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['child_age'] = np.int16(data['child_age'])"
      ],
      "metadata": {
        "id": "D-OzNM4F1kD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.boxplot(data=data, x='child_age', y='CBCL_score')\n",
        "\n"
      ],
      "metadata": {
        "id": "stQFrO4I0DpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
        "\n",
        "# child_gender == 1\n",
        "sns.boxplot(\n",
        "    data=data[data['child_gender'] == 1],\n",
        "    x='child_age', y='CBCL_score',\n",
        "    ax=axes[0]\n",
        ")\n",
        "axes[0].set_title('Child Gender = 1')\n",
        "\n",
        "# child_gender == 2\n",
        "sns.boxplot(\n",
        "    data=data[data['child_gender'] == 2],\n",
        "    x='child_age', y='CBCL_score',\n",
        "    ax=axes[1]\n",
        ")\n",
        "axes[1].set_title('Child Gender = 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JvJJAYVQ29wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"CBCL mean: {data['CBCL_score'].mean():.5}, CBCL SD: {data['CBCL_score'].std():.5}\")\n",
        "print(f\"CBCL D mean: {data['CBCL_D_score'].mean():.5}, CBCL D SD: {data['CBCL_D_score'].std():.5}\")\n",
        "print(f\"CBCL A mean: {data['CBCL_A_score'].mean():.5}, CBCL A SD: {data['CBCL_A_score'].std():.5}\")\n",
        "print(f\"CBCL S mean: {data['CBCL_S_score'].mean():.5}, CBCL S SD: {data['CBCL_S_score'].std():.5}\")\n"
      ],
      "metadata": {
        "id": "R8OhnX0NU0Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data[['self_exposure_score', 'other_exposure_score', 'war_exposure_score','CBCL_D_score', 'CBCL_A_score', 'CBCL_S_score', 'CBCL_score', 'Event_length', 'EER_text_length']])"
      ],
      "metadata": {
        "id": "tefSXHlnvWag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = data[['self_exposure_score', 'other_exposure_score', 'war_exposure_score','CBCL_D_score', 'CBCL_A_score', 'CBCL_S_score', 'CBCL_score', 'Event_length', 'EER_text_length']].corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(8, 6)) # Adjust figure size as needed\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4D89uJfmt9OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Stanza (Run this once)\n",
        "!pip install stanza"
      ],
      "metadata": {
        "id": "5J0IPymKx3r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import pandas as pd\n",
        "\n",
        "# 2. Download the Hebrew model (Run this once)\n",
        "# This downloads the pre-trained neural network for Hebrew\n",
        "stanza.download('he')\n",
        "\n",
        "# 3. Initialize the pipeline\n",
        "# We need 'tokenize' (split words), 'mwt' (multi-word token expansion), and 'lemma' (base forms)\n",
        "nlp = stanza.Pipeline(lang='he', processors='tokenize,mwt,pos,lemma', use_gpu=True)\n",
        "\n",
        "# 4. Define the Lemmatization Function\n",
        "def get_hebrew_lemmas(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "\n",
        "    # Process the text with Stanza\n",
        "    doc = nlp(text)\n",
        "\n",
        "    lemmas = []\n",
        "    # Stanza organizes text into sentences -> words\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            # 'word.lemma' is the base form (e.g., 'הלכנו' -> 'הלך')\n",
        "            # We filter out punctuation manually if needed\n",
        "            if word.upos != 'PUNCT':\n",
        "                lemmas.append(word.lemma)\n",
        "    return lemmas\n",
        "\n",
        "# --- APPLY TO YOUR DATAFRAME ---\n",
        "# Assuming your dataframe is called 'data' and the text column is 'text_column'\n",
        "\n",
        "# (Replace 'text_column' with the actual name of your column, e.g., 'description')\n",
        "column_name = 'Event'\n",
        "\n",
        "# This might take a few minutes depending on the size of your data\n",
        "print(\"Processing text... this may take a while for large datasets.\")\n",
        "data['lemmas'] = data[column_name].apply(get_hebrew_lemmas)\n",
        "\n",
        "# --- INSPECT RESULTS ---\n",
        "print(data[[column_name, 'lemmas']].head())\n",
        "\n",
        "# Example of what this achieves:\n",
        "# Input: \"הילדים בכו כשהלכנו לממ\"ד\"\n",
        "# Output (Lemmas): ['ה', 'ילד', 'בכה', 'כש', 'הלך', 'ל', 'ה', 'ממ\"ד']\n",
        "# Note how 'בכו' became 'בכה' and 'כשהלכנו' was split and normalized to 'הלך'."
      ],
      "metadata": {
        "id": "i9Wi5QdNwvzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Sentence Transformers (Run this once)\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "k9Nvq_8GKidh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['EER_text'].notna()]"
      ],
      "metadata": {
        "id": "q8ZFsxsRMi5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import re\n",
        "\n",
        "\n",
        "# --- STEP A: Encode Text to Vectors ---\n",
        "# We use a multilingual model that supports Hebrew well.\n",
        "# This replaces the need to manually average Word2Vec vectors.\n",
        "print(\"Loading Embedding Model...\")\n",
        "model_name = 'intfloat/multilingual-e5-base'\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "\n",
        "# Ensure we are using the original Hebrew text column (not the lemmas)\n",
        "# for context-aware embeddings.\n",
        "# REPLACE 'text_column' with your actual column name.\n",
        "text_col = ['Event', 'EER_text']\n",
        "target_col = 'CBCL_score'\n",
        "\n",
        "# Encode the text\n",
        "print(\"Encoding text features... (This may take a moment)\")\n",
        "# The model converts each text entry into a vector of numbers (e.g., 384 dimensions)\n",
        "X_Event_embeddings = embedding_model.encode(data[text_col[0]].tolist(), show_progress_bar=True)\n",
        "X_EER_embeddings = embedding_model.encode(data[text_col[1]].tolist(), show_progress_bar=True)\n",
        "\n",
        "X_embeddings = np.hstack([X_Event_embeddings, X_EER_embeddings])\n",
        "\n",
        "# def embed_hebrew(texts):\n",
        "#     all_vecs = []\n",
        "#     for t in texts:\n",
        "#         parts = re.split(r\"[.!?…]\", t)\n",
        "#         parts = [p.strip() for p in parts if len(p.strip()) > 5]\n",
        "\n",
        "#         # E5 requires this prefix\n",
        "#         parts = [\"query: \" + p for p in parts]\n",
        "\n",
        "#         vecs = embedding_model.encode(\n",
        "#             parts,\n",
        "#             normalize_embeddings=True\n",
        "#         )\n",
        "#         all_vecs.append(vecs.mean(axis=0))\n",
        "#     return np.vstack(all_vecs)\n",
        "\n",
        "\n",
        "# X_embeddings = embed_hebrew(data[\"Event\"].tolist())\n",
        "\n",
        "\n",
        "# # Scale features, as Ridge is sensitive to feature scale\n",
        "# from sklearn.preprocessing import normalize\n",
        "# X_embeddings = normalize(X_embeddings)\n",
        "\n",
        "\n",
        "# Prepare Target\n",
        "y = data[target_col].values\n",
        "\n",
        "# Split Data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "o9eoNsxLCOnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP B: Train Model with Hyperparameter Optimization ---\n",
        "\n",
        "# We will use Ridge Regression (Linear Model).\n",
        "# It works well with high-dimensional embedding data and prevents overfitting.\n",
        "regressor = Ridge()\n",
        "\n",
        "# Define Hyperparameters to tune\n",
        "# alpha: Regularization strength (higher = stronger regularization)\n",
        "param_grid = {\n",
        "    \"alpha\": np.logspace(-4, 1, 25)\n",
        "}\n",
        "\n",
        "\n",
        "print(\"\\n--- Tuning Hyperparameters (Grid Search) ---\")\n",
        "grid_search = GridSearchCV(\n",
        "    Ridge(),\n",
        "    param_grid,\n",
        "    cv=10,\n",
        "    scoring=\"r2\"\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Train the best model on full training set (GridSearch does this automatically, but to be explicit)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# --- STEP C: Evaluate against Naive Predictor ---\n",
        "\n",
        "# 1. The Naive Predictor (Baseline)\n",
        "# \"Naive\" usually means predicting the Mean or Median for everyone.\n",
        "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
        "dummy_regr.fit(X_train, y_train)\n",
        "\n",
        "# 2. Make Predictions\n",
        "y_pred_model = best_model.predict(X_test)\n",
        "y_pred_naive = dummy_regr.predict(X_test)\n",
        "\n",
        "# 3. Calculate Metrics\n",
        "rmse_model = np.sqrt(mean_squared_error(y_test, y_pred_model))\n",
        "rmse_naive = np.sqrt(mean_squared_error(y_test, y_pred_naive))\n",
        "\n",
        "r2_model = r2_score(y_test, y_pred_model)\n",
        "r2_naive = r2_score(y_test, y_pred_naive)\n",
        "\n",
        "# 4. Display Results\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"      MODEL EVALUATION RESULTS      \")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Metric':<20} | {'Naive Predictor':<15} | {'Tuned Linear Model':<15}\")\n",
        "print(\"-\" * 56)\n",
        "print(f\"{'RMSE (Lower is better)':<20} | {rmse_naive:.4f}          | {rmse_model:.4f}\")\n",
        "print(f\"{'R2 (Higher is better)':<20} | {r2_naive:.4f}          | {r2_model:.4f}\")\n",
        "print(\"-\" * 56)\n",
        "\n",
        "if rmse_model < rmse_naive:\n",
        "    print(\"\\nSUCCESS: Your text-based model outperforms the naive guess!\")\n",
        "else:\n",
        "    print(\"\\nRESULT: The model did not beat the naive guess. Consider different features or models.\")"
      ],
      "metadata": {
        "id": "r7mkj2BmKatm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=500,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "xD918vl1Ep6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Calculate Metrics\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
        "\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# 4. Display Results\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"      MODEL EVALUATION RESULTS      \")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Metric':<20} | {'Naive Predictor':<15} | {'Random Forest Model':<15}\")\n",
        "print(\"-\" * 56)\n",
        "print(f\"{'RMSE (Lower is better)':<20} | {rmse_naive:.4f}          | {rmse_rf:.4f}\")\n",
        "print(f\"{'R2 (Higher is better)':<20} | {r2_naive:.4f}          | {r2_rf:.4f}\")\n",
        "print(\"-\" * 56)\n",
        "\n",
        "if rmse_model < rmse_naive:\n",
        "    print(\"\\nSUCCESS: Your text-based model outperforms the naive guess!\")\n",
        "else:\n",
        "    print(\"\\nRESULT: The model did not beat the naive guess. Consider different features or models.\")"
      ],
      "metadata": {
        "id": "v8chqSByEvci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP A: Encode Text to Vectors ---\n",
        "# We use a multilingual model that supports Hebrew well.\n",
        "# This replaces the need to manually average Word2Vec vectors.\n",
        "print(\"Loading Embedding Model...\")\n",
        "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "# Ensure we are using the original Hebrew text column (not the lemmas)\n",
        "# for context-aware embeddings.\n",
        "# REPLACE 'text_column' with your actual column name.\n",
        "text_col = 'lemmas'\n",
        "target_col = 'CBCL_score'\n",
        "\n",
        "# Encode the text\n",
        "print(\"Encoding text features... (This may take a moment)\")\n",
        "# The model converts each text entry into a vector of numbers (e.g., 384 dimensions)\n",
        "X_embeddings = embedding_model.encode(data[text_col].tolist(), show_progress_bar=True)\n",
        "\n",
        "# Prepare Target\n",
        "y = data[target_col].values\n",
        "\n",
        "# Split Data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- STEP B: Train Model with Hyperparameter Optimization ---\n",
        "\n",
        "# We will use Ridge Regression (Linear Model).\n",
        "# It works well with high-dimensional embedding data and prevents overfitting.\n",
        "regressor = Ridge()\n",
        "\n",
        "# Define Hyperparameters to tune\n",
        "# alpha: Regularization strength (higher = stronger regularization)\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 1.0, 10.0, 100.0, 200.0]\n",
        "}\n",
        "\n",
        "print(\"\\n--- Tuning Hyperparameters (Grid Search) ---\")\n",
        "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Train the best model on full training set (GridSearch does this automatically, but to be explicit)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# --- STEP C: Evaluate against Naive Predictor ---\n",
        "\n",
        "# 1. The Naive Predictor (Baseline)\n",
        "# \"Naive\" usually means predicting the Mean or Median for everyone.\n",
        "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
        "dummy_regr.fit(X_train, y_train)\n",
        "\n",
        "# 2. Make Predictions\n",
        "y_pred_model = best_model.predict(X_test)\n",
        "y_pred_naive = dummy_regr.predict(X_test)\n",
        "\n",
        "# 3. Calculate Metrics\n",
        "rmse_model = np.sqrt(mean_squared_error(y_test, y_pred_model))\n",
        "rmse_naive = np.sqrt(mean_squared_error(y_test, y_pred_naive))\n",
        "\n",
        "r2_model = r2_score(y_test, y_pred_model)\n",
        "r2_naive = r2_score(y_test, y_pred_naive)\n",
        "\n",
        "# 4. Display Results\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"      MODEL EVALUATION RESULTS      \")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Metric':<20} | {'Naive Predictor':<15} | {'Tuned Linear Model':<15}\")\n",
        "print(\"-\" * 56)\n",
        "print(f\"{'RMSE (Lower is better)':<20} | {rmse_naive:.4f}          | {rmse_model:.4f}\")\n",
        "print(f\"{'R2 (Higher is better)':<20} | {r2_naive:.4f}          | {r2_model:.4f}\")\n",
        "print(\"-\" * 56)\n",
        "\n",
        "if rmse_model < rmse_naive:\n",
        "    print(\"\\nSUCCESS: Your text-based model outperforms the naive guess!\")\n",
        "else:\n",
        "    print(\"\\nRESULT: The model did not beat the naive guess. Consider different features or models.\")"
      ],
      "metadata": {
        "id": "FJzlkdTq9pcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TEXT_COL_1 = 'event_description' # Replace with actual column name\n",
        "TEXT_COL_2 = 'letter_to_self'    # Replace with actual column name\n",
        "TARGET_COL = 'CBCL_score'\n",
        "\n",
        "# 1. Load Model\n",
        "print(\"Loading Embedding Model...\")\n",
        "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "# 2. Encode Column 1 (Event)\n",
        "print(f\"Encoding {TEXT_COL_1}...\")\n",
        "# Fill NaNs with empty string to prevent errors\n",
        "texts_1 = data[TEXT_COL_1].fillna('').tolist()\n",
        "embeddings_1 = embedding_model.encode(texts_1, show_progress_bar=True)\n",
        "\n",
        "# 3. Encode Column 2 (Letter)\n",
        "print(f\"Encoding {TEXT_COL_2}...\")\n",
        "texts_2 = data[TEXT_COL_2].fillna('').tolist()\n",
        "embeddings_2 = embedding_model.encode(texts_2, show_progress_bar=True)\n",
        "\n",
        "# 4. Concatenate Vectors\n",
        "# Instead of one vector of size 384, we now have a combined vector of size 768 per person.\n",
        "print(\"combining vectors...\")\n",
        "X_combined = np.hstack((embeddings_1, embeddings_2))\n",
        "\n",
        "# Prepare Target\n",
        "y = data[TARGET_COL].values\n",
        "\n",
        "# 5. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- TRAIN & EVALUATE (Same as before) ---\n",
        "\n",
        "regressor = Ridge()\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0, 200.0, 500.0]} # Added higher alpha for more features\n",
        "\n",
        "print(\"\\n--- Tuning Hyperparameters ---\")\n",
        "grid_search = GridSearchCV(regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Baseline\n",
        "dummy = DummyRegressor(strategy=\"mean\")\n",
        "dummy.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_naive = dummy.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "rmse_val = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "rmse_naive = np.sqrt(mean_squared_error(y_test, y_naive))\n",
        "r2_val = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"      EVALUATION RESULTS (Dual Vectors)      \")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Metric':<20} | {'Naive':<15} | {'Dual-Vector Model':<15}\")\n",
        "print(\"-\" * 56)\n",
        "print(f\"{'RMSE':<20} | {rmse_naive:.4f}          | {rmse_val:.4f}\")\n",
        "print(f\"{'R2':<20} | {'0.0000'}          | {r2_val:.4f}\")\n",
        "print(\"-\" * 56)"
      ],
      "metadata": {
        "id": "AFgQRnyFzEaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Use 'max_depth' to prevent overfitting\n",
        "rf_model = RandomForestRegressor(n_estimators=120, max_depth=70, random_state=42)\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred = rf_model.predict(X_test)\n",
        "# Check R2 score for this new model"
      ],
      "metadata": {
        "id": "h2HPTILqK1YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Metrics\n",
        "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "rmse_naive = np.sqrt(mean_squared_error(y_test, y_pred_naive))\n",
        "r2_rf = r2_score(y_test, y_pred)\n",
        "\n",
        "# 4. Display Results\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"      MODEL EVALUATION RESULTS      \")\n",
        "print(\"=\"*40)\n",
        "print(f\"{'Metric':<20} | {'Naive Predictor':<15} | {'Random Forest Model':<15}\")\n",
        "print(\"-\" * 56)\n",
        "print(f\"{'RMSE (Lower is better)':<20} | {rmse_naive:.4f}          | {rmse_rf:.4f}\")\n",
        "print(f\"{'R2 (Higher is better)':<20} | {r2_naive:.4f}          | {r2_rf:.4f}\")\n",
        "print(\"-\" * 56)"
      ],
      "metadata": {
        "id": "pZCbTsD5Cqb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ns1Emh7uK2Es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "puf7PWRoK3G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# 1. Define Hebrew Stopwords & Prefixes to Ignore\n",
        "# This list includes single-letter prefixes (handled by Stanza splitting)\n",
        "# and common function words irrelevant to the semantic analysis.\n",
        "ignore_words = {\n",
        "    # Prefixes (often separated by Stanza)\n",
        "    'ה', 'ו', 'ב', 'ל', 'ש', 'מ', 'כ', 'כש',\n",
        "    # Pronouns & Function words\n",
        "    'אני', 'את', 'אתה', 'אנחנו', 'הוא', 'היא', 'הם', 'הן',\n",
        "    'זה', 'זאת', 'אלו', 'של', 'על', 'עם', 'כל', 'רק', 'אבל',\n",
        "    'או', 'אם', 'גם', 'לא', 'כן', 'כי', 'אז', 'יותר', 'פחות',\n",
        "    'היה', 'הייתה', 'היו', 'יש', 'אין', 'לי', 'לו', 'לה', 'לנו'\n",
        "}\n",
        "\n",
        "def filter_prefixes(lemma_list):\n",
        "    \"\"\"\n",
        "    Filters out prefixes, single letters, and stopwords.\n",
        "    Returns a clean list of content words.\n",
        "    \"\"\"\n",
        "    if not isinstance(lemma_list, list):\n",
        "        return []\n",
        "\n",
        "    # Keep word if:\n",
        "    # 1. It is not in the ignore list\n",
        "    # 2. It is longer than 1 character (removes remaining detached prefixes)\n",
        "    # 3. It is alphabetical (removes punctuation)\n",
        "    return [word for word in lemma_list\n",
        "            if word not in ignore_words\n",
        "            and len(word) > 1\n",
        "            and word.isalpha()]\n",
        "\n",
        "# 2. Apply Filtering\n",
        "# Assumes 'lemmas' is the column from the previous Stanza step\n",
        "data['clean_content'] = data['lemmas'].apply(filter_prefixes)\n",
        "\n",
        "# 3. Flatten list for global statistics\n",
        "all_words = [word for sublist in data['clean_content'] for word in sublist]\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# --- VISUALIZATION 1: Top 20 Most Frequent Content Words ---\n",
        "\n",
        "# Get top 20 words\n",
        "top_words = word_counts.most_common(20)\n",
        "words = [w[0] for w in top_words]\n",
        "counts = [w[1] for w in top_words]\n",
        "\n",
        "# Reverse Hebrew strings for correct display in Matplotlib\n",
        "words_reversed = [w[::-1] for w in words]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=counts, y=words_reversed, palette='viridis')\n",
        "plt.title('Top 20 Frequent Content Words (Prefixes Removed)')\n",
        "plt.xlabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# --- VISUALIZATION 2: Word Cloud ---\n",
        "\n",
        "# Prepare text for WordCloud (Reversing strings individually for display)\n",
        "# We join them with spaces\n",
        "text_for_cloud = \" \".join([w[::-1] for w in all_words])\n",
        "\n",
        "# Generate\n",
        "wc = WordCloud(width=800, height=400, background_color='white', font_path=None).generate(text_for_cloud)\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Semantic Content\")\n",
        "plt.show()\n",
        "\n",
        "# --- VISUALIZATION 3: Distribution of Description Length ---\n",
        "# This checks how much \"content\" (meaningful words) participants wrote.\n",
        "# Short responses might correlate with avoidance or lower engagement.\n",
        "\n",
        "data['content_length'] = data['clean_content'].apply(len)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data['content_length'], bins=30, kde=True, color='purple')\n",
        "plt.title('Distribution of Content Word Count per Response')\n",
        "plt.xlabel('Number of Meaningful Words')\n",
        "plt.ylabel('Number of Participants')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yWnOjb_6zO21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define specific words to investigate (Lemmatized forms)\n",
        "# These align with the CBCL subscales mentioned in your papers.\n",
        "target_words = [\n",
        "    'פחד',    # Fear (Internalizing)\n",
        "    'בכה',    # Cry (Internalizing)\n",
        "    'כעס',    # Anger (Externalizing)\n",
        "    'צעק',    # Shout (Externalizing)\n",
        "    'מצב',  # Hit (Externalizing)\n",
        "    'זמן',   # Cling (Internalizing/Anxiety)\n",
        "    'לבד',    # Alone (Isolation)\n",
        "    'קשה'     # Hard/Difficult (General Distress)\n",
        "]\n",
        "\n",
        "# 2. Extract Features for these words only\n",
        "# We assume 'clean_content' contains the list of lemmas from the previous step.\n",
        "# If you only have the text column, use data['text_column'] and searching strings.\n",
        "\n",
        "analysis_data = []\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    # Flatten the list of lemmas for this participant\n",
        "    lemmas = row['clean_content']\n",
        "\n",
        "    # Base dictionary with the target score\n",
        "    row_stats = {\n",
        "        'CBCL_score': row['CBCL_score'] # Ensure this matches your column name exactly\n",
        "    }\n",
        "\n",
        "    # Calculate stats for each target word\n",
        "    for word in target_words:\n",
        "        count = lemmas.count(word)\n",
        "        row_stats[f'count_{word}'] = count\n",
        "        row_stats[f'has_{word}'] = \"Yes\" if count > 0 else \"No\"\n",
        "\n",
        "    analysis_data.append(row_stats)\n",
        "\n",
        "df_analysis = pd.DataFrame(analysis_data)\n",
        "\n",
        "# --- VISUALIZATION 1: IMPACT OF APPEARANCE (Box Plots) ---\n",
        "# Question: \"Is the CBCL score higher when the word appears?\"\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# We create subplots for each word\n",
        "for i, word in enumerate(target_words):\n",
        "    plt.subplot(2, 4, i+1) # Adjust grid size (2 rows, 4 columns) based on list length\n",
        "\n",
        "    sns.boxplot(x=f'has_{word}', y='CBCL_score', data=df_analysis,\n",
        "                order=['No', 'Yes'], palette={'No': 'skyblue', 'Yes': 'salmon'})\n",
        "\n",
        "    plt.title(f'Word: {word[::-1]}') # Reverse Hebrew for title\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('CBCL Score' if i % 4 == 0 else '') # Only show label on left plots\n",
        "\n",
        "plt.suptitle('Impact of Word Appearance on CBCL Score', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- VISUALIZATION 2: IMPACT OF INCREASING FREQUENCY (Line/Bar Plots) ---\n",
        "# Question: \"Does the score go up as the word is used more?\"\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Choose just one or two strong words to visualize detailed frequency trends\n",
        "# Otherwise the plot gets too messy. Let's look at 'fear' (פחד) and 'anger' (כעס).\n",
        "focus_words = ['פחד', 'כעס']\n",
        "\n",
        "for word in focus_words:\n",
        "    # Calculate Mean CBCL score for each frequency (0 times, 1 time, 2 times...)\n",
        "    # We group 3+ occurrences together to avoid noisy outliers with small samples.\n",
        "\n",
        "    df_analysis[f'freq_group_{word}'] = df_analysis[f'count_{word}'].apply(lambda x: str(x) if x < 3 else '3+')\n",
        "\n",
        "    # Sort order for x-axis\n",
        "    order = ['0', '1', '2', '3+']\n",
        "\n",
        "    # Group and plot\n",
        "    sns.lineplot(x=f'freq_group_{word}', y='CBCL_score', data=df_analysis,\n",
        "                 marker='o', label=word[::-1], errorbar='se') # 'se' shows Standard Error confidence interval\n",
        "\n",
        "plt.title('Mean CBCL Score vs. Word Frequency')\n",
        "plt.xlabel('Number of times word appears in text')\n",
        "plt.ylabel('Mean CBCL Score')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# --- STATISTICAL CHECK (T-Test) ---\n",
        "# Quickly check if the difference between \"Yes\" and \"No\" groups is significant\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "print(\"--- Statistical Significance (T-Test: Word Present vs. Absent) ---\")\n",
        "for word in target_words:\n",
        "    group_yes = df_analysis[df_analysis[f'has_{word}'] == 'Yes']['CBCL_score']\n",
        "    group_no = df_analysis[df_analysis[f'has_{word}'] == 'No']['CBCL_score']\n",
        "\n",
        "    if len(group_yes) > 0 and len(group_no) > 0:\n",
        "        t_stat, p_val = ttest_ind(group_yes, group_no, equal_var=False)\n",
        "        sig = \"**\" if p_val < 0.05 else \"\"\n",
        "        print(f\"Word '{word[::-1]}': p-value = {p_val:.4f} {sig}\")"
      ],
      "metadata": {
        "id": "sBDUnsFYElTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# 1. Identify the Top 20 Most Frequent Words\n",
        "# We use the 'clean_content' column (lemmas without prefixes) from the previous step.\n",
        "# Ensure you run the \"filter_prefixes\" step before this.\n",
        "all_lemmas = [word for sublist in data['clean_content'] for word in sublist]\n",
        "top_20_words = [word for word, count in Counter(all_lemmas).most_common(20)]\n",
        "\n",
        "# 2. Calculate Statistics for Each Word\n",
        "analysis_data = []\n",
        "stats_list = []\n",
        "\n",
        "print(f\"{'Word':<15} | {'Mean Score (Present)':<20} | {'Mean Score (Absent)':<20} | {'P-Value':<10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for word in top_20_words:\n",
        "    # Create a mask: True if the word appears in the parent's text, False otherwise\n",
        "    has_word = data['clean_content'].apply(lambda x: word in x)\n",
        "\n",
        "    # Split the CBCL scores into two groups\n",
        "    scores_present = data.loc[has_word, 'CBCL_score']\n",
        "    scores_absent = data.loc[~has_word, 'CBCL_score']\n",
        "\n",
        "    # Calculate Means\n",
        "    mean_present = scores_present.mean()\n",
        "    mean_absent = scores_absent.mean()\n",
        "\n",
        "    # Perform T-Test (only if we have enough data points, e.g., >5 in each group)\n",
        "    if len(scores_present) > 5 and len(scores_absent) > 5:\n",
        "        t_stat, p_val = ttest_ind(scores_present, scores_absent, equal_var=False)\n",
        "    else:\n",
        "        p_val = 1.0 # Not significant if data is insufficient\n",
        "\n",
        "    # Store data for plotting\n",
        "    diff = mean_present - mean_absent  # Positive diff = Word linked to HIGHER score\n",
        "    stats_list.append({\n",
        "        'word': word,\n",
        "        'mean_present': mean_present,\n",
        "        'mean_absent': mean_absent,\n",
        "        'diff': diff,\n",
        "        'p_val': p_val\n",
        "    })\n",
        "\n",
        "    # Print simplified table\n",
        "    sig_mark = \"**\" if p_val < 0.05 else \"\"\n",
        "    print(f\"{word[::-1]:<15} | {mean_present:<20.2f} | {mean_absent:<20.2f} | {p_val:.4f} {sig_mark}\")\n",
        "\n",
        "# 3. Visualization: Difference in Means\n",
        "stats_df = pd.DataFrame(stats_list)\n",
        "stats_df = stats_df.sort_values('diff', ascending=False) # Sort: Risk words on top\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Color coding: Red for significant Risk, Green for significant Protective, Gray for insignificant\n",
        "colors = []\n",
        "for _, row in stats_df.iterrows():\n",
        "    if row['p_val'] < 0.05:\n",
        "        # Significant\n",
        "        colors.append('salmon' if row['diff'] > 0 else 'lightgreen')\n",
        "    else:\n",
        "        # Not Significant\n",
        "        colors.append('lightgray')\n",
        "\n",
        "# Reverse Hebrew labels for correct display\n",
        "labels_reversed = [w[::-1] for w in stats_df['word']]\n",
        "\n",
        "# Create Bar Plot\n",
        "sns.barplot(x='diff', y=labels_reversed, data=stats_df, palette=colors)\n",
        "\n",
        "plt.axvline(0, color='black', linestyle='--')\n",
        "plt.title('Impact of Top 20 Common Words on CBCL Score\\n(Difference in Mean Score: Present vs. Absent)', fontsize=14)\n",
        "plt.xlabel('Difference in CBCL Score Points (Positive = Risk, Negative = Protective)')\n",
        "plt.ylabel('Word')\n",
        "\n",
        "# Manual Legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='salmon', label='Significantly Higher Score (Risk Factor)'),\n",
        "    Patch(facecolor='lightgreen', label='Significantly Lower Score (Resilience Factor)'),\n",
        "    Patch(facecolor='lightgray', label='Not Significant')\n",
        "]\n",
        "plt.legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bkls5KXn8_B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TARGET_COL = 'CBCL_score'  # <--- REPLACE with your actual score column name\n",
        "MIN_DOC_FREQ = 5           # Ignore words appearing in fewer than 5 responses\n",
        "\n",
        "# 1. Prepare Data for Vectorization\n",
        "# CountVectorizer expects a list of strings (sentences), not a list of lists.\n",
        "# We join your filtered lemmas back into strings.\n",
        "corpus = data['clean_content'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# 2. Vectorize (Create the \"Bag of Words\")\n",
        "# We only keep words that appear in at least MIN_DOC_FREQ documents to filter noise.\n",
        "vectorizer = CountVectorizer(min_df=MIN_DOC_FREQ)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Create a DataFrame of word counts\n",
        "word_counts_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# 3. Calculate Correlations\n",
        "# We assume the index of word_counts_df matches 'data'. Reset index to be safe.\n",
        "word_counts_df.index = data.index\n",
        "\n",
        "# Add the target score to the word dataframe temporarily to calculate corr\n",
        "word_counts_df['__TARGET__'] = data[TARGET_COL]\n",
        "\n",
        "# Calculate correlation of every word with the target\n",
        "correlations = word_counts_df.corr()['__TARGET__'].drop('__TARGET__')\n",
        "\n",
        "# 4. Extract Top Positive and Negative Correlations\n",
        "top_positive = correlations.sort_values(ascending=False).head(10)\n",
        "top_negative = correlations.sort_values(ascending=True).head(10)\n",
        "\n",
        "print(\"--- Top Words Associated with HIGH CBCL Scores (Risk?) ---\")\n",
        "print(top_positive)\n",
        "print(\"\\n--- Top Words Associated with LOW CBCL Scores (Resilience?) ---\")\n",
        "print(top_negative)\n",
        "\n",
        "# --- VISUALIZATION: Correlation Bar Plot ---\n",
        "\n",
        "# Combine top pos/neg for a single plot\n",
        "top_corr = pd.concat([top_positive, top_negative])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Reverse Hebrew labels for display\n",
        "labels_reversed = [w[::-1] for w in top_corr.index]\n",
        "\n",
        "sns.barplot(x=top_corr.values, y=labels_reversed, palette='coolwarm')\n",
        "plt.title(f'Correlation between Word Usage and {TARGET_COL}', fontsize=14)\n",
        "plt.xlabel('Pearson Correlation Coefficient')\n",
        "plt.axvline(0, color='black', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# --- VISUALIZATION: Specific Word Impact (Box Plot) ---\n",
        "# Let's verify if a specific \"high risk\" word actually separates the groups.\n",
        "# We take the #1 most positively correlated word.\n",
        "\n",
        "if not top_positive.empty:\n",
        "    risk_word = top_positive.index[0] # The word with highest correlation\n",
        "\n",
        "    # Create a binary column: Did the parent use this word? Yes/No\n",
        "    data['has_risk_word'] = data['clean_content'].apply(lambda x: risk_word in x)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(data=data, x='has_risk_word', y=TARGET_COL, palette='Set2')\n",
        "    plt.title(f'Effect of using the word \"{risk_word[::-1]}\" on {TARGET_COL}')\n",
        "    plt.xlabel(f'Contains word \"{risk_word[::-1]}\"?')\n",
        "    plt.ylabel('CBCL Score')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NrTsZBIF_JnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Event_words_lst'] = data['Event'].apply(clean_text_basic).apply(tokenize_hebrew)\n",
        "\n",
        "\n",
        "data['Event_words_lst']"
      ],
      "metadata": {
        "id": "60yixOdjFTpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Event_top_10_words'] = data['Event_words_lst'].apply(get_top_n_words)\n",
        "\n",
        "data['Event_top_10_words']"
      ],
      "metadata": {
        "id": "ubCgMYPG7WGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "# Flatten all words in Event_top_10_words\n",
        "# Each row is a list of (word, count) tuples, we only take the word\n",
        "all_top_words = list(chain.from_iterable([[w for w, c in lst] for lst in data['Event_top_10_words']]))\n",
        "\n",
        "# Count frequency\n",
        "word_counter = Counter(all_top_words)\n",
        "\n",
        "# Get the 10 most common words\n",
        "top10_words_overall = word_counter.most_common(10)\n",
        "\n",
        "print(\"Top 10 most common words across all events:\")\n",
        "for word, freq in top10_words_overall:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "id": "7RLbcNPE84QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_events_txt = ''\n",
        "\n",
        "for txt in data['Event']:\n",
        "  all_events_txt += txt"
      ],
      "metadata": {
        "id": "9W-h8KJ69Ixu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_top_n_words(tokenize_hebrew(all_events_txt), n=10)"
      ],
      "metadata": {
        "id": "Su9MxpHu9XJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List of top 10 words\n",
        "top_words = [\"ילדים\",\"שלי\",\"בית\",\"אזעקה\",\"חוסר\",\"ילד\",\"חמה\",\"הבן\",\"הבת\",\"לי\",\"שאני\",\"מצב\",\"שלא\"]\n",
        "\n",
        "# Create binary columns: 1 if word appears in Event_words_lst, 0 otherwise\n",
        "for word in top_words:\n",
        "    data[f'word_{word}'] = data['Event_words_lst'].apply(lambda lst: int(word in lst))\n",
        "\n",
        "# Compare mean CBCL for events with vs without each word\n",
        "comparison = []\n",
        "for word in top_words:\n",
        "    col = f'word_{word}'\n",
        "    mean_with = data.loc[data[col] == 1, 'CBCL_score'].mean()\n",
        "    mean_without = data.loc[data[col] == 0, 'CBCL_score'].mean()\n",
        "    comparison.append({\n",
        "        'word': word,\n",
        "        'CBCL_with_word': mean_with,\n",
        "        'CBCL_without_word': mean_without\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison)\n",
        "\n",
        "# Melt for plotting\n",
        "comparison_melted = comparison_df.melt(\n",
        "    id_vars='word',\n",
        "    value_vars=['CBCL_with_word', 'CBCL_without_word'],\n",
        "    var_name='Condition', value_name='CBCL_score'\n",
        ")\n",
        "\n",
        "# Clean names for plot\n",
        "comparison_melted['Condition'] = comparison_melted['Condition'].map({\n",
        "    'CBCL_with_word': 'With Word',\n",
        "    'CBCL_without_word': 'Without Word'\n",
        "})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=comparison_melted, x='CBCL_score', y='word', hue='Condition')\n",
        "plt.xlabel('Mean CBCL Score')\n",
        "plt.ylabel('Word')\n",
        "plt.title('CBCL Scores by Presence of Top 10 Event Words')\n",
        "plt.legend(title='')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "48XQ0J1z-8cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create binary columns: 1 if word appears in Event_words_lst, 0 otherwise\n",
        "for word in top_words:\n",
        "    data[f'word_{word}'] = data['Event_words_lst'].apply(lambda lst: int(word in lst))\n",
        "\n",
        "# Compare mean war_exposure_score for events with vs without each word\n",
        "comparison = []\n",
        "for word in top_words:\n",
        "    col = f'word_{word}'\n",
        "    mean_with = data.loc[data[col] == 1, 'war_exposure_score'].mean()\n",
        "    mean_without = data.loc[data[col] == 0, 'war_exposure_score'].mean()\n",
        "    comparison.append({\n",
        "        'word': word,\n",
        "        'WarExposure_with_word': mean_with,\n",
        "        'WarExposure_without_word': mean_without\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison)\n",
        "\n",
        "# Melt for plotting\n",
        "comparison_melted = comparison_df.melt(\n",
        "    id_vars='word',\n",
        "    value_vars=['WarExposure_with_word', 'WarExposure_without_word'],\n",
        "    var_name='Condition', value_name='WarExposure_score'\n",
        ")\n",
        "\n",
        "# Clean names for plot\n",
        "comparison_melted['Condition'] = comparison_melted['Condition'].map({\n",
        "    'WarExposure_with_word': 'With Word',\n",
        "    'WarExposure_without_word': 'Without Word'\n",
        "})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=comparison_melted, x='WarExposure_score', y='word', hue='Condition')\n",
        "plt.xlabel('Mean War Exposure Score')\n",
        "plt.ylabel('Word')\n",
        "plt.title('War Exposure Scores by Presence of Top 10 Event Words')\n",
        "plt.legend(title='')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pgwRfegzBEPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "rma6Sblu6ZW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "blbga0c5_UMM"
      }
    }
  ]
}